{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Ethan Buckner - Data Project 1\n",
    "\n",
    "I used the sample database 'sakila', provided by MySQL.\n",
    "\n",
    "We start by establishing the credentials to a local MySQL server:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import pymysql.cursors\n",
    "import pymongo\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "mysql_host_name = \"localhost\"\n",
    "mysql_host_ip = \"127.0.0.1\"\n",
    "mysql_port = \"3306\"\n",
    "mysql_user_id = \"root\"\n",
    "mysql_pwd = \"king in the north\"\n",
    "\n",
    "atlas_cluster_name = \"EthanCluster\"\n",
    "atlas_user_name = \"root\"\n",
    "atlas_password = \"kinginthenorth\"\n",
    "\n",
    "conn_str = {\"local\" : f\"mongodb://localhost:27017/\",\n",
    "    \"atlas\" : \"mongodb+srv://root:kinginthenorth@ethancluster.tnalhre.mongodb.net/?retryWrites=true&w=majority\"\n",
    "}\n",
    "\n",
    "src_dbname = \"sakila\"\n",
    "dst_dbname = \"sakila_fact\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I decided to drop the column 'last_update' from all tables because the data was not necessary and the timestamp data type would be difficult to work with."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "conn = pymysql.connect(host=mysql_host_name, user=mysql_user_id, password=mysql_pwd, database=src_dbname)\n",
    "cursor = conn.cursor(pymysql.cursors.DictCursor)\n",
    "cursor.execute(\"ALTER TABLE actor DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE address DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE category DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE city DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE country DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE customer DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE film DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE film_actor DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE film_category DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE inventory DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE language DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE payment DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE rental DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE staff DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE store DROP COLUMN last_update;\")\n",
    "cursor.close()\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I then executed the queries in sakila-schema.sql and sakila-data.sql using MySQLWorkbench. I also created my destination database: sakila_fact. I used the script provided to create a dim_date table and populate it with dates from 1/1/2001 to 12/31/2024.\n",
    "\n",
    "I now need to edit all of my tables that include date information (customer, payment, and rental) to store dim_date foreign keys rather than date objects. I decided to use pandas dataframes to do this rather than sql."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def get_dataframe(user_id, pwd, host_name, db_name, sql_query):\n",
    "    conn_str = f\"mysql+pymysql://{user_id}:{pwd}@{host_name}/{db_name}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    dframe = pd.read_sql(text(sql_query), connection);\n",
    "    connection.close()\n",
    "\n",
    "    return dframe\n",
    "\n",
    "customer_df = get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, src_dbname, \"SELECT * FROM sakila.customer;\")\n",
    "payment_df = get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, src_dbname, \"SELECT * FROM sakila.payment;\")\n",
    "rental_df = get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, src_dbname, \"SELECT * FROM sakila.rental;\")\n",
    "\n",
    "dim_date = get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, dst_dbname, \"SELECT * FROM sakila_fact.dim_date;\")\n",
    "dim_date['full_date'] = dim_date['full_date'].astype(str)\n",
    "\n",
    "customer_df['create_date'] = customer_df['create_date'].astype(str).str[:10]\n",
    "customer_date = customer_df.merge(dim_date, left_on='create_date', right_on='full_date')\n",
    "customer_df = customer_df.drop(columns=['create_date'])\n",
    "customer_df['create_date_id'] = customer_date['date_key']\n",
    "\n",
    "payment_df['payment_date'] = payment_df['payment_date'].astype(str).str[:10]\n",
    "payment_date = payment_df.merge(dim_date, left_on='payment_date', right_on='full_date')\n",
    "payment_df = payment_df.drop(columns=['payment_date'])\n",
    "payment_df['payment_date_id'] = payment_date['date_key']\n",
    "\n",
    "rental_df['rental_date'] = rental_df['rental_date'].astype(str).str[:10]\n",
    "rental_date = rental_df.merge(dim_date, left_on='rental_date', right_on='full_date')\n",
    "rental_df = rental_df.drop(columns=['rental_date', 'return_date'])\n",
    "rental_df['rental_date_id'] = rental_date['date_key']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we will extract the tables related to information about the store involved in a transaction. This includes the tables: 'address', 'city', 'country', and 'store'. We will then store the contents of these tables in a MongoDB database."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<pymongo.results.InsertManyResult at 0x127511d60>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "address_df = get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, src_dbname, \"SELECT * FROM sakila.address;\")\n",
    "address_df = address_df.drop(columns=['location', 'address2'])\n",
    "\n",
    "city_df = get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, src_dbname, \"SELECT * FROM sakila.city;\")\n",
    "country_df = get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, src_dbname, \"SELECT * FROM sakila.country;\")\n",
    "store_df = get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, src_dbname, \"SELECT * FROM sakila.store;\")\n",
    "\n",
    "client = pymongo.MongoClient(conn_str['atlas'], server_api=ServerApi('1'))\n",
    "db = client['sakila']\n",
    "db['address'].insert_many(address_df.to_dict(orient='records'))\n",
    "db['city'].insert_many(city_df.to_dict(orient='records'))\n",
    "db['country'].insert_many(country_df.to_dict(orient='records'))\n",
    "db['store'].insert_many(store_df.to_dict(orient='records'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now I will extract the rental table and store the data in a csv file in a file called rental.csv."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "rental_df.to_csv('rental.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now I have data from three of the different data sources required, pulling tables from MySQL, MongoDB, and a local csv file. Now I will put all the relevant tables into dataframes, create a fact dataframe for rentals, then save the dim tables and fact table to the sakila_fact MySQL db."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      order_pk           film_title  amount staff_last_name  \\\n",
      "0            0   GALAXY SWEETHEARTS    4.99        Stephens   \n",
      "1            1  LIBERTY MAGNIFICENT    8.99        Stephens   \n",
      "2            2    NECKLACE OUTBREAK    5.99        Stephens   \n",
      "3            3        CLERKS ANGELS    6.99        Stephens   \n",
      "4            4      HOLLOW JEOPARDY    4.99        Stephens   \n",
      "...        ...                  ...     ...             ...   \n",
      "2014      2014    CONTACT ANONYMOUS    2.99         Hillyer   \n",
      "2015      2015     WASTELAND DIVINE    2.99         Hillyer   \n",
      "2016      2016           LION UNCUT    3.99         Hillyer   \n",
      "2017      2017      HIGHBALL POTTER    0.99         Hillyer   \n",
      "2018      2018            TAXI KICK    0.99         Hillyer   \n",
      "\n",
      "     customer_last_name                      customer_email  \\\n",
      "0                  LONG  JACQUELINE.LONG@sakilacustomer.org   \n",
      "1                  LONG  JACQUELINE.LONG@sakilacustomer.org   \n",
      "2                  LONG  JACQUELINE.LONG@sakilacustomer.org   \n",
      "3                  LONG  JACQUELINE.LONG@sakilacustomer.org   \n",
      "4                  LONG  JACQUELINE.LONG@sakilacustomer.org   \n",
      "...                 ...                                 ...   \n",
      "2014             MEEHAN      CORY.MEEHAN@sakilacustomer.org   \n",
      "2015             MEEHAN      CORY.MEEHAN@sakilacustomer.org   \n",
      "2016            GAFFNEY    FELIX.GAFFNEY@sakilacustomer.org   \n",
      "2017            GAFFNEY    FELIX.GAFFNEY@sakilacustomer.org   \n",
      "2018            GAFFNEY    FELIX.GAFFNEY@sakilacustomer.org   \n",
      "\n",
      "          customer_address customer_city customer_country  rental_id  \\\n",
      "0        870 Ashqelon Loop      Songkhla         Thailand       3270   \n",
      "1        870 Ashqelon Loop      Songkhla         Thailand      10252   \n",
      "2        870 Ashqelon Loop      Songkhla         Thailand       8803   \n",
      "3        870 Ashqelon Loop      Songkhla         Thailand      10584   \n",
      "4        870 Ashqelon Loop      Songkhla         Thailand      15099   \n",
      "...                    ...           ...              ...        ...   \n",
      "2014       556 Asuncin Way      Mogiljov          Belarus       6050   \n",
      "2015       556 Asuncin Way      Mogiljov          Belarus       1398   \n",
      "2016  1059 Yuncheng Avenue       Vilnius        Lithuania       3050   \n",
      "2017  1059 Yuncheng Avenue       Vilnius        Lithuania       9336   \n",
      "2018  1059 Yuncheng Avenue       Vilnius        Lithuania       6459   \n",
      "\n",
      "      customer_id  customer_address_id  staff_id  film_id  store_id  \\\n",
      "0              86                   90         2      346         2   \n",
      "1              86                   90         2      519         2   \n",
      "2              86                   90         2      618         2   \n",
      "3              86                   90         2      156         2   \n",
      "4              86                   90         2      422         2   \n",
      "...           ...                  ...       ...      ...       ...   \n",
      "2014          527                  533         1      181         1   \n",
      "2015          527                  533         1      962         1   \n",
      "2016          557                  563         1      524         1   \n",
      "2017          557                  563         1      416         1   \n",
      "2018          557                  563         1      877         1   \n",
      "\n",
      "      payment_date_id  rental_date_id  \n",
      "0            20050709        20050621  \n",
      "1            20050709        20050801  \n",
      "2            20050709        20050729  \n",
      "3            20050709        20050801  \n",
      "4            20050709        20050822  \n",
      "...               ...             ...  \n",
      "2014         20050620        20050711  \n",
      "2015         20050620        20050615  \n",
      "2016         20050706        20050620  \n",
      "2017         20050706        20050730  \n",
      "2018         20050706        20050712  \n",
      "\n",
      "[2019 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "# MongoDB tables\n",
    "address_df = pd.DataFrame(list(db['address'].find()))\n",
    "city_df = pd.DataFrame(list(db['city'].find()))\n",
    "country_df = pd.DataFrame(list(db['country'].find()))\n",
    "store_df = pd.DataFrame(list(db['store'].find()))\n",
    "\n",
    "# Drop ids made by MongoDB\n",
    "address_df = address_df.drop(columns=[\"_id\"])\n",
    "city_df = city_df.drop(columns=[\"_id\"])\n",
    "country_df = country_df.drop(columns=[\"_id\"])\n",
    "store_df = store_df.drop(columns=[\"_id\"])\n",
    "\n",
    "# CSV table\n",
    "rental_df = pd.read_csv('rental.csv')\n",
    "\n",
    "# MySQL tables\n",
    "# Already have customer_df and payment_df\n",
    "inventory_df = get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, src_dbname, \"SELECT * FROM sakila.inventory;\")\n",
    "film_df = get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, src_dbname, \"SELECT * FROM sakila.film;\")\n",
    "staff_df = get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, src_dbname, \"SELECT staff_id, first_name, last_name, address_id, email, store_id FROM sakila.staff;\")\n",
    "\n",
    "fact_rentals = payment_df.merge(rental_df, on=['rental_id', 'staff_id', 'customer_id']).drop(columns=[\"payment_id\"]) # No need to store these ids because all data is now in fact table\n",
    "fact_rentals = fact_rentals.merge(inventory_df, on='inventory_id').drop(columns=[\"inventory_id\"])\n",
    "fact_rentals = fact_rentals.merge(film_df, on='film_id').drop(columns=[\"description\", \"release_year\", \"language_id\", \"original_language_id\", \"rental_duration\", \"rental_rate\", \"length\", \"replacement_cost\", \"rating\", \"special_features\"]).rename(columns={\"title\": \"film_title\"})\n",
    "fact_rentals = fact_rentals.merge(customer_df, on=['customer_id', 'store_id']).drop(columns=[\"first_name\", \"active\", \"create_date_id\"]).rename(columns={\"email\": \"customer_email\", \"address_id\": \"customer_address_id\", \"last_name\": \"customer_last_name\"})\n",
    "fact_rentals = fact_rentals.merge(address_df, left_on='customer_address_id', right_on='address_id').drop(columns=[\"district\", \"postal_code\", \"phone\", \"address_id\"])\n",
    "fact_rentals = fact_rentals.merge(city_df, on='city_id')\n",
    "fact_rentals = fact_rentals.merge(country_df, on='country_id').drop(columns=['city_id', 'country_id']).rename(columns={'address': 'customer_address', 'city': 'customer_city', 'country': 'customer_country'})\n",
    "fact_rentals = fact_rentals.merge(staff_df, on=['staff_id', 'store_id']).drop(columns=['address_id', 'email', 'first_name']).rename(columns={'last_name': 'staff_last_name'})\n",
    "\n",
    "reordered = ['film_title', 'amount', 'staff_last_name', 'customer_last_name', 'customer_email', 'customer_address', 'customer_city', 'customer_country', 'rental_id', 'customer_id', 'customer_address_id', 'staff_id', 'film_id', 'store_id', 'payment_date_id', 'rental_date_id']\n",
    "\n",
    "fact_rentals = fact_rentals[reordered]\n",
    "fact_rentals.reset_index(inplace=True)\n",
    "fact_rentals = fact_rentals.rename(columns={\"index\": \"order_pk\"})\n",
    "dim_address = address_df.drop(columns=[\"phone\"])\n",
    "dim_city = city_df\n",
    "dim_country = country_df\n",
    "dim_store = store_df\n",
    "dim_staff = staff_df\n",
    "dim_film = film_df\n",
    "dim_inventory = inventory_df\n",
    "dim_customer = customer_df\n",
    "\n",
    "print(fact_rentals)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have our fact and dimension tables, lets save them to our sakila_fact database."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def set_dataframe(user_id, pwd, host_name, db_name, df, table_name, pk_column, db_operation):\n",
    "    conn_str = f\"mysql+pymysql://{user_id}:{pwd}@{host_name}/{db_name}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    with sqlEngine.connect() as conn:\n",
    "        if db_operation == \"insert\":\n",
    "            df.to_sql(table_name, con=conn, index=False, if_exists='replace')\n",
    "            conn.execute(text(f\"ALTER TABLE {table_name} ADD PRIMARY KEY ({pk_column});\"))\n",
    "\n",
    "        elif db_operation == \"update\":\n",
    "            df.to_sql(table_name, con=conn, index=False, if_exists='append')\n",
    "\n",
    "set_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, dst_dbname, fact_rentals, \"fact_rentals\", \"order_pk\", \"insert\")\n",
    "set_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, dst_dbname, dim_address, \"dim_address\", \"address_id\", \"insert\")\n",
    "set_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, dst_dbname, dim_city, \"dim_city\", \"city_id\", \"insert\")\n",
    "set_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, dst_dbname, dim_country, \"dim_country\", \"country_id\", \"insert\")\n",
    "set_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, dst_dbname, dim_store, \"dim_store\", \"store_id\", \"insert\")\n",
    "set_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, dst_dbname, dim_staff, \"dim_staff\", \"staff_id\", \"insert\")\n",
    "set_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, dst_dbname, dim_film, \"dim_film\", \"film_id\", \"insert\")\n",
    "set_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, dst_dbname, dim_inventory, \"dim_inventory\", \"inventory_id\", \"insert\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Proof of functionality: Get total amount spent on rentals for each customer email. Get all rentals with less than an R rating checked out by staff with the first name of Mike"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              film_title  amount first_name rating\n",
      "0      FELLOWSHIP AUTUMN    4.99       Mike  NC-17\n",
      "1          VAMPIRE WHALE    7.99       Mike  NC-17\n",
      "2         SWEDEN SHINING    7.99       Mike     PG\n",
      "3    LEAGUE HELLFIGHTERS    8.99       Mike  PG-13\n",
      "4    COLDBLOODED DARLING    5.99       Mike      G\n",
      "..                   ...     ...        ...    ...\n",
      "841       IMAGE PRINCESS    8.99       Mike  PG-13\n",
      "842    CONTACT ANONYMOUS    2.99       Mike  PG-13\n",
      "843     WASTELAND DIVINE    2.99       Mike     PG\n",
      "844           LION UNCUT    3.99       Mike     PG\n",
      "845            TAXI KICK    0.99       Mike  PG-13\n",
      "\n",
      "[846 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "spent_sql = \"SELECT customer_email, SUM(amount) AS total_spent FROM sakila_fact.fact_rentals GROUP BY customer_email ORDER BY total_spent;\"\n",
    "\n",
    "total_spent = get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, dst_dbname, spent_sql)\n",
    "\n",
    "print(total_spent)\n",
    "\n",
    "mike_family_sql = \"SELECT fact_rentals.film_title, fact_rentals.amount, dim_staff.first_name, dim_film.rating FROM sakila_fact.fact_rentals JOIN sakila_fact.dim_staff ON sakila_fact.fact_rentals.staff_id = sakila_fact.dim_staff.staff_id JOIN sakila_fact.dim_film ON sakila_fact.fact_rentals.film_id = sakila_fact.dim_film.film_id WHERE dim_film.rating != 'R' AND dim_staff.first_name = 'Mike';\"\n",
    "\n",
    "mike_family_df = get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, dst_dbname, mike_family_sql)\n",
    "\n",
    "print(mike_family_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
